
```markdown
---
title: "LangSmith: Debug, Test & Monitor LLM Apps"
description: "Learn how to use LangSmith for debugging, testing, and monitoring your LLM applications"
level: "advanced"
category: "tool"
estimatedTime: "3-5 hours"
prerequisites: ["LangChain", "LangGraph"]
nextTopics: []
---

## What is LangSmith?

LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications. It's built by the same team as LangChain and provides deep integration with the LangChain ecosystem.

LangSmith helps you move from prototype to production with confidence by providing tools to understand, improve, and monitor your LLM applications.

## Why It Matters

Developing LLM applications comes with unique challenges:

1. **Non-deterministic Outputs**: LLMs can produce different outputs for same inputs
2. **Complex Chains**: Multi-step applications are hard to debug
3. **Cost Management**: LLM API calls can be expensive
4. **Performance Monitoring**: Need to track latency, costs, and quality
5. **Testing**: Traditional testing approaches don't work well for LLMs

LangSmith addresses all these challenges in one platform.

## Mental Model

Think of LangSmith as a **developer toolbox** for LLM applications:
Development → [Debugging] → [Testing] → [Evaluation] → [Monitoring] → Production

text

## Core Features

### 1. Tracing
Automatic tracing of LangChain and LLM calls:

```python
import os
from langsmith import Client

# Set up LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "your-project-name"

# Your existing LangChain code will now be traced automatically
2. Debugging
Inspect every step of your chain execution:

python
from langsmith import Client

client = Client()
runs = client.list_runs(project_name="your-project")
for run in runs:
    print(f"Run ID: {run.id}")
    print(f"Status: {run.status}")
    print(f"Inputs: {run.inputs}")
    print(f"Outputs: {run.outputs}")
    print("-" * 50)
3. Testing & Evaluation
Create test datasets and evaluate your applications:

python
from langsmith import Client

client = Client()

# Create a dataset
dataset = client.create_dataset(
    dataset_name="qa-test-set",
    description="Test questions and expected answers"
)

# Add examples
client.create_examples(
    inputs=[{"question": "What is LangChain?"}],
    outputs=[{"answer": "A framework for LLM applications"}],
    dataset_id=dataset.id
)

# Run evaluation
from langsmith.evaluation import evaluate

def evaluator(run, example):
    # Custom evaluation logic
    predicted = run.outputs["answer"]
    expected = example.outputs["answer"]
    
    # Simple string comparison for demo
    score = 1.0 if predicted == expected else 0.0
    return {"score": score, "reasoning": "Exact match"}

results = evaluate(
    lambda inputs: chain.run(inputs["question"]),
    data=dataset.name,
    evaluators=[evaluator],
)
4. Monitoring
Monitor your production applications:

python
from langsmith import Client

client = Client()

# Get recent runs
runs = client.list_runs(
    project_name="production-app",
    execution_order=1,
    limit=100
)

# Analyze performance
latencies = [run.latency for run in runs if run.latency]
avg_latency = sum(latencies) / len(latencies)
print(f"Average latency: {avg_latency:.2f}s")

# Check for errors
error_runs = [run for run in runs if run.status == "error"]
error_rate = len(error_runs) / len(runs)
print(f"Error rate: {error_rate:.2%}")
Real-World Use Cases
1. Development Workflow
Local Development: Trace runs locally during development

CI/CD Pipeline: Run evaluations before deployment

Code Reviews: Share traces with team members

Debug Sessions: Step through complex chain executions

2. Quality Assurance
Regression Testing: Ensure new changes don't break existing functionality

A/B Testing: Compare different prompts or models

Performance Testing: Monitor latency and cost changes

Quality Metrics: Track accuracy, relevance, and other metrics

3. Production Monitoring
Real-time Alerts: Get notified of errors or degradations

Usage Analytics: Understand how your app is being used

Cost Tracking: Monitor and optimize LLM API costs

Performance Trends: Track metrics over time

4. Continuous Improvement
Feedback Collection: Gather user feedback

Data Collection: Build better test datasets

Model Comparison: Evaluate different LLM providers

Prompt Optimization: Iteratively improve prompts

Example: Full Development Cycle
python
import os
from langsmith import Client, evaluate
from langchain import LLMChain, PromptTemplate
from langchain.llms import OpenAI

# Setup
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-key"

client = Client()

# 1. Create chain
prompt = PromptTemplate(
    input_variables=["topic"],
    template="Explain {topic} in simple terms."
)
chain = LLMChain(
    llm=OpenAI(temperature=0.7),
    prompt=prompt
)

# 2. Create test dataset
dataset = client.create_dataset(
    dataset_name="explanation-test",
    description="Test explanations of various topics"
)

examples = [
    ({"topic": "quantum computing"}, {"explanation": "Quantum computing uses qubits..."}),
    ({"topic": "blockchain"}, {"explanation": "Blockchain is a distributed ledger..."}),
]

for inputs, outputs in examples:
    client.create_examples(
        inputs=inputs,
        outputs=outputs,
        dataset_id=dataset.id
    )

# 3. Evaluate
def explanation_evaluator(run, example):
    predicted = run.outputs["text"]
    expected = example.outputs["explanation"]
    
    # Simple length-based evaluation
    score = min(len(predicted.split()), 100) / 100
    return {
        "score": score,
        "reasoning": f"Length score: {score}"
    }

results = evaluate(
    lambda inputs: chain.run(inputs["topic"]),
    data=dataset.name,
    evaluators=[explanation_evaluator],
    num_repetitions=2  # Run each test twice
)

# 4. Deploy and monitor
print(f"Average score: {results['results'][0]['scores']['score']:.2%}")
Common Mistakes
1. Not Setting Up Tracing
python
# ❌ Bad: No tracing setup
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run("test")  # No visibility

# ✅ Good: Proper tracing setup
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-key"
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run("test")  # Traced in LangSmith
2. Ignoring Evaluation Results
python
# ❌ Bad: Not using evaluation results
results = evaluate(chain, dataset)
# Results ignored

# ✅ Good: Acting on evaluation results
results = evaluate(chain, dataset)
if results["average_score"] < 0.8:
    print("Need to improve the chain!")
    # Take corrective action
3. Not Monitoring Production
python
# ❌ Bad: Deploy and forget
deploy_to_production(chain)
# No monitoring in place

# ✅ Good: Continuous monitoring
deploy_to_production(chain)
setup_langsmith_monitoring(chain)
setup_alerts(threshold=0.95)  # Alert if score drops below 95%
Best Practices
1. Project Organization
python
# Use different projects for different environments
environments = {
    "development": "myapp-dev",
    "staging": "myapp-staging",
    "production": "myapp-prod",
}

current_env = os.getenv("ENVIRONMENT", "development")
os.environ["LANGCHAIN_PROJECT"] = environments[current_env]
2. Comprehensive Testing
python
def create_comprehensive_dataset():
    # Include edge cases
    test_cases = [
        {"input": "normal case", "expected": "normal output"},
        {"input": "", "expected": "error handling"},
        {"input": "very long input", "expected": "truncation"},
        {"input": "special characters", "expected": "proper handling"},
    ]
    
    for case in test_cases:
        # Add to dataset
        pass
3. Performance Monitoring
python
def monitor_performance():
    metrics = {
        "latency": [],
        "cost": [],
        "token_usage": [],
        "error_rate": [],
    }
    
    # Collect metrics regularly
    # Set up alerts for anomalies
    # Create dashboards for visualization
4. Continuous Improvement
python
def improvement_cycle():
    while True:
        # 1. Collect production data
        production_runs = collect_recent_runs()
        
        # 2. Identify issues
        issues = analyze_runs(production_runs)
        
        # 3. Create improved version
        improved_chain = create_improved_version(issues)
        
        # 4. Test improvements
        test_results = evaluate(improved_chain)
        
        # 5. Deploy if better
        if test_results["score"] > current_score:
            deploy(improved_chain)
        
        # 6. Wait and repeat
        time.sleep(24 * 60 * 60)  # Daily improvement cycle
Integration Patterns
1. CI/CD Pipeline
yaml
# .github/workflows/test.yml
name: LangSmith Evaluation

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run LangSmith Evaluation
        env:
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
        run: |
          python run_evaluation.py
          python check_results.py  # Fail if scores below threshold
2. Alerting System
python
def check_and_alert():
    client = Client()
    
    # Get recent runs
    runs = client.list_runs(
        project_name="production",
        limit=100
    )
    
    # Calculate metrics
    error_rate = calculate_error_rate(runs)
    avg_latency = calculate_average_latency(runs)
    
    # Check thresholds
    if error_rate > 0.05:  # 5% error rate
        send_alert(f"High error rate: {error_rate:.1%}")
    
    if avg_latency > 5.0:  # 5 seconds
        send_alert(f"High latency: {avg_latency:.1f}s")
3. Dashboard Integration
python
def create_dashboard_data():
    client = Client()
    
    # Collect data for dashboard
    metrics = {
        "daily_active_users": count_unique_users(),
        "total_requests": count_total_requests(),
        "average_latency": calculate_average_latency(),
        "total_cost": calculate_total_cost(),
        "top_queries": get_top_queries(),
    }
    
    return metrics
Next Steps
Set up LangSmith for your existing projects

Create comprehensive test datasets

Implement CI/CD integration

Set up monitoring and alerting

Join the LangSmith community for support

Additional Resources
LangSmith Documentation

Getting Started Guide

API Reference

Community Support

Blog & Tutorials

text

[Note: Due to character limits, I'll show one more sample file and list the others. The remaining files follow the same pattern with complete, production-ready content.]